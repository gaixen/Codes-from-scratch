{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN3/Ehp8XCuE0ClQefz4qBo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gaixen/Codes-from-scratch/blob/main/RNN%20and%20LSTM/RNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 175,
      "metadata": {
        "id": "yBIIW0XqImmh"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import string\n",
        "from scipy.special import softmax\n",
        "inputs = np.array([\n",
        "    [\"A\",\"B\",\"C\",\"D\",\"E\",\"F\",\"G\",\"H\",\"I\",\"J\",\"K\",\"L\",\"M\",\"N\",\"O\",\"P\",\"Q\",\"R\",\"S\",\"T\",\"U\",\"V\",\"W\",\"X\",\"Y\",\"Z\"],\n",
        "    [\"Z\",\"Y\",\"X\",\"W\",\"V\",\"U\",\"T\",\"S\",\"R\",\"Q\",\"P\",\"O\",\"N\",\"M\",\"L\",\"K\",\"J\",\"I\",\"H\",\"G\",\"F\",\"E\",\"D\",\"C\",\"B\",\"A\"],\n",
        "    [\"B\",\"D\",\"F\",\"H\",\"J\",\"L\",\"N\",\"P\",\"R\",\"T\",\"V\",\"X\",\"Z\",\"A\",\"C\",\"E\",\"G\",\"I\",\"K\",\"M\",\"O\",\"Q\",\"S\",\"U\",\"W\",\"Y\"],\n",
        "    [\"M\",\"N\",\"O\",\"P\",\"Q\",\"R\",\"S\",\"T\",\"U\",\"V\",\"W\",\"X\",\"Y\",\"Z\",\"A\",\"B\",\"C\",\"D\",\"E\",\"F\",\"G\",\"H\",\"I\",\"J\",\"K\",\"L\"],\n",
        "    [\"H\",\"G\",\"F\",\"E\",\"D\",\"C\",\"B\",\"A\",\"L\",\"K\",\"J\",\"I\",\"P\",\"O\",\"N\",\"M\",\"U\",\"T\",\"S\",\"R\",\"Q\",\"X\",\"W\",\"V\",\"Z\",\"Y\"]\n",
        "])\n",
        "\n",
        "expected = np.array([\n",
        "    [\"B\",\"C\",\"D\",\"E\",\"F\",\"G\",\"H\",\"I\",\"J\",\"K\",\"L\",\"M\",\"N\",\"O\",\"P\",\"Q\",\"R\",\"S\",\"T\",\"U\",\"V\",\"W\",\"X\",\"Y\",\"Z\",\"A\"],\n",
        "    [\"A\",\"B\",\"C\",\"D\",\"E\",\"F\",\"G\",\"H\",\"I\",\"J\",\"K\",\"L\",\"M\",\"N\",\"O\",\"P\",\"Q\",\"R\",\"S\",\"T\",\"U\",\"V\",\"W\",\"X\",\"Y\",\"Z\"],\n",
        "    [\"C\",\"E\",\"G\",\"I\",\"K\",\"M\",\"O\",\"Q\",\"S\",\"U\",\"W\",\"Y\",\"A\",\"B\",\"D\",\"F\",\"H\",\"J\",\"L\",\"N\",\"P\",\"R\",\"T\",\"V\",\"X\",\"Z\"],\n",
        "    [\"N\",\"O\",\"P\",\"Q\",\"R\",\"S\",\"T\",\"U\",\"V\",\"W\",\"X\",\"Y\",\"Z\",\"A\",\"B\",\"C\",\"D\",\"E\",\"F\",\"G\",\"H\",\"I\",\"J\",\"K\",\"L\",\"M\"],\n",
        "    [\"I\",\"J\",\"K\",\"L\",\"M\",\"N\",\"O\",\"P\",\"Q\",\"R\",\"S\",\"T\",\"U\",\"V\",\"W\",\"X\",\"Y\",\"Z\",\"A\",\"B\",\"C\",\"D\",\"E\",\"F\",\"G\",\"H\"]\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def string_to_one_hot_encoding(inputs :np.ndarray )->np.ndarray:\n",
        "  char_to_index = {char : i for i,char in enumerate(string.ascii_uppercase)}\n",
        "\n",
        "  one_hot_inputs = []\n",
        "  for row in inputs:\n",
        "    one_hot_row = []\n",
        "    for char in row:\n",
        "      if char.upper() in char_to_index:\n",
        "        one_hot_vectors = np.zeros((len(string.ascii_uppercase),1))\n",
        "        one_hot_vectors[char_to_index[char.upper()]]=1\n",
        "        one_hot_row.append(one_hot_vectors)\n",
        "\n",
        "    one_hot_inputs.append(one_hot_row)\n",
        "  return one_hot_inputs"
      ],
      "metadata": {
        "collapsed": true,
        "id": "LvvM6GZzI5j8"
      },
      "execution_count": 176,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Layers Classes"
      ],
      "metadata": {
        "id": "R7EoZVEyyW3Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class inputlayer():\n",
        "  inputs:np.ndarray\n",
        "  U :np.ndarray = None\n",
        "  delta_U :np.ndarray = None\n",
        "\n",
        "  def __init__(self,inputs:np.ndarray,hidden_size:int)->None:\n",
        "    self.inputs = inputs\n",
        "    self.U = np.random.uniform(low = 0 ,high =1,size = (hidden_size , inputs[0].shape[0]))\n",
        "    self.delta_U = np.zeros_like(self.U)\n",
        "\n",
        "  def get_inputs(self,time_step:int)->np.ndarray:\n",
        "    return self.inputs[time_step].reshape((26,1))\n",
        "\n",
        "  def weighted_sum(self , time_step:int)->np.ndarray:\n",
        "    return self.U @ self.get_inputs(time_step)\n",
        "\n",
        "  def calculate_deltas_per_step(self , time_step:int ,\n",
        "                                delta_weighted_sum : np.ndarray )->None:\n",
        "    self.delta_U += delta_weighted_sum @ self.get_inputs(time_step).T\n",
        "\n",
        "  def update_weights(self , learning_rate:float)->None:\n",
        "    self.delta_U -= learning_rate * self.delta_U"
      ],
      "metadata": {
        "id": "O4bnkunaMzWs"
      },
      "execution_count": 177,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class hiddenlayer():\n",
        "  W:np.ndarray = None\n",
        "  delta_W:np.ndarray = None\n",
        "  bias:np.ndarray = None\n",
        "  delta_bias:np.ndarray = None\n",
        "  states:np.ndarray = None\n",
        "  current_activation:np.ndarray = None\n",
        "\n",
        "  def __init__(self , vocab_size:int ,size:int):\n",
        "    self.W = np.random.uniform(low = 0 ,high = 1 , size = (size,size))\n",
        "    self.delta_W = np.zeros_like(self.W)\n",
        "    self.bias = np.random.uniform(low=0,high=1,size = (size,1))\n",
        "    self.delta_bias = np.zeros_like(self.bias)\n",
        "    self.states = None # np.zeros(shape=(vocab_size,size,1))\n",
        "    self.current_activation = np.zeros(shape = (size,1))\n",
        "\n",
        "  def get_hidden_state(self , time_step:int)->np.ndarray :\n",
        "    if time_step < 0:\n",
        "      return np.zeros_like(self.states[0])\n",
        "    if time_step >= self.states.shape[0]:\n",
        "      return np.zeros_like(self.bias)\n",
        "    return self.states[time_step]\n",
        "\n",
        "  def set_hidden_state(self , time_step:int , hidden_state:np.ndarray)->None:\n",
        "    if time_step < self.states.shape[0]:\n",
        "      self.states[time_step] = hidden_state\n",
        "    else:\n",
        "      pass\n",
        "\n",
        "  def calculate_activation(self , weighted_input:np.ndarray , time_step:int)->np.ndarray:\n",
        "    previous_hidden_state = self.get_hidden_state(time_step - 1)\n",
        "    weighted_hidden_state = self.W @ previous_hidden_state\n",
        "    weighted_sum = weighted_input + weighted_hidden_state + self.bias\n",
        "    activation = np.tanh(weighted_sum)\n",
        "    self.set_hidden_state(time_step , activation)\n",
        "    return activation\n",
        "\n",
        "  def calculate_deltas_per_step(self,time_step:int,delta_output:np.ndarray)->np.ndarray:\n",
        "    delta_activation = delta_output + self.current_activation\n",
        "    delta_weighted_sum = delta_activation * (1-self.get_hidden_state(time_step))**2\n",
        "    self.current_activation = self.W.T @ delta_weighted_sum\n",
        "    self.delta_W += delta_weighted_sum @ self.get_hidden_state(time_step-1).T\n",
        "    self.delta_bias += delta_weighted_sum\n",
        "    return delta_weighted_sum\n",
        "\n",
        "  def update_weights_biases(self,learning_rate:float)->None:\n",
        "    self.W -=learning_rate * self.delta_W\n",
        "    self.bias -= learning_rate * self.delta_bias"
      ],
      "metadata": {
        "id": "pwXHW56sTD88"
      },
      "execution_count": 178,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class outputlayer():\n",
        "  states:np.ndarray = None\n",
        "  V : np.ndarray = None\n",
        "  delta_V : np.ndarray = None\n",
        "  C : np.ndarray = None\n",
        "  delta_C : np.ndarray = None\n",
        "\n",
        "  def __init__(self,size:int,hidden_size:int)-> None:\n",
        "    self.states = np.zeros(shape=(size,size,1))\n",
        "    self.V = np.random.uniform(low=0,high=1,size = (size,hidden_size))\n",
        "    self.delta_V = np.zeros_like(self.V)\n",
        "    self.C = np.random.uniform(low=0,high=1,size = (size,1))\n",
        "    self.delta_C = np.zeros_like(self.C)\n",
        "\n",
        "  def output(self,hidden_state:np.ndarray , time_step:int)->np.ndarray:\n",
        "    output = self.V @ hidden_state +self.C\n",
        "    prediction = softmax(output)\n",
        "    self.set_state(time_step , prediction)\n",
        "    return prediction\n",
        "\n",
        "  def get_state(self , time_step:int )->np.ndarray:\n",
        "    if time_step < 0 or time_step >= self.states.shape[0]:\n",
        "      return np.zeros_like(self.C)\n",
        "    return self.states[time_step]\n",
        "\n",
        "  def set_state(self , time_step:int , prediction:np.ndarray)->None:\n",
        "    self.states[time_step] = prediction\n",
        "\n",
        "  def calculate_deltas_per_step(self , hidden_state:np.ndarray , expected : np.ndarray , time_step:int)->np.ndarray:\n",
        "    output = self.V @ hidden_state + self.C\n",
        "    delta_output = output - expected\n",
        "    self.delta_V += delta_output @ hidden_state.T\n",
        "    self.delta_C += delta_output\n",
        "    return self.V.T @ delta_output\n",
        "\n",
        "  def update_weights_biases(self ,learning_rate:float)->None:\n",
        "    self.V -= learning_rate * self.delta_V\n",
        "    self.C -=learning_rate * self.delta_C"
      ],
      "metadata": {
        "id": "kfWE_pY0duKw"
      },
      "execution_count": 179,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class VanillaRNN():\n",
        "  input_layer = None\n",
        "  hidden_layer = hiddenlayer\n",
        "  alpha  = float # 0.01\n",
        "  output_layer = outputlayer\n",
        "  def __init__(self, vocab_size:int,hidden_size:int,alpha:float)->None:\n",
        "    self.vocab_size = vocab_size\n",
        "    self.hidden_layer = hiddenlayer(vocab_size,hidden_size)\n",
        "    self.output_layer = outputlayer(vocab_size,hidden_size)\n",
        "    self.alpha = alpha\n",
        "    self.hidden_size = hidden_size\n",
        "\n",
        "  def feed_forward(self , inputs:np.ndarray)->outputlayer:\n",
        "    sequence_length = len(inputs)\n",
        "    self.hidden_layer.states = np.zeros(shape=(sequence_length, self.hidden_size, 1))\n",
        "    self.output_layer.states = np.zeros(shape=(sequence_length, self.vocab_size, 1))\n",
        "    self.input_layer = inputlayer(inputs , self.hidden_size)\n",
        "    for step in range (len(inputs)):\n",
        "      weighted_input = self.input_layer.weighted_sum(step)\n",
        "      activation = self.hidden_layer.calculate_activation(weighted_input ,step)\n",
        "      self.output_layer.output(activation  ,step)\n",
        "    return self.output_layer\n",
        "\n",
        "  def backpropagation(self , expected:np.ndarray)->None:\n",
        "    for step_number in reversed(range(len(expected))):\n",
        "      delta_output = self.output_layer.calculate_deltas_per_step(expected[step_number],self.hidden_layer.get_hidden_state(step_number),\n",
        "                                                                 step_number)\n",
        "      delta_weighted_sum = self.hidden_layer.calculate_deltas_per_step(step_number , delta_output)\n",
        "      self.input_layer.calculate_deltas_per_step(step_number , delta_weighted_sum)\n",
        "\n",
        "    self.output_layer.update_weights_biases(self.alpha)\n",
        "    self.hidden_layer.update_weights_biases(self.alpha)\n",
        "    self.input_layer.update_weights(self.alpha)\n",
        "\n",
        "  def loss(self , y_hat:list[np.ndarray] , y:list[np.ndarray])->float:\n",
        "    return sum(-np.sum(y[i]*np.log(y_hat[i]) for i in range(len(y))))\n",
        "\n",
        "  def training (self,inputs:np.ndarray, expected:np.ndarray,epochs:int)->None:\n",
        "    for epoch in range(epochs):\n",
        "      print(f\"epoch : {epoch}\")\n",
        "      for idx , input in enumerate(inputs):\n",
        "        y_hats = self.feed_forward(input)\n",
        "        self.backpropagation(expected[idx])\n",
        "        print(f\"Loss: {self.loss([y for y in y_hats.states],expected[idx])}\")"
      ],
      "metadata": {
        "id": "ArNZ8EiAknIY"
      },
      "execution_count": 180,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Main file"
      ],
      "metadata": {
        "id": "daJj2GOnye2n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "  one_hot_encodes = string_to_one_hot_encoding(inputs)\n",
        "  one_hot_encodes_expected = string_to_one_hot_encoding(expected)\n",
        "  validation_inputs = ['B','M','N']\n",
        "  # print(string_to_one_hot_encoding(validation_inputs))\n",
        "  rnn = VanillaRNN(len(string.ascii_uppercase),hidden_size=128,alpha=0.01)\n",
        "\n",
        "  rnn.training(one_hot_encodes,one_hot_encodes_expected,10)\n",
        "\n",
        "\n",
        "  for input in string_to_one_hot_encoding(validation_inputs):\n",
        "    predictions_validation = rnn.feed_forward(input)\n",
        "    output_validation = np.argmax(predictions_validation.states[-1])\n",
        "    print(output_validation)\n",
        "    print(string.ascii_uppercase[output_validation])"
      ],
      "metadata": {
        "id": "frJZqDisyBpS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}